<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/README.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/README.md" />
              <option name="originalContent" value="#  Production Ready RAG System&#10;&#10;A production-grade Retrieval Augmented Generation (RAG) system built with FastAPI, Inngest, Qdrant, and Google Gemini. This system allows you to ingest PDF documents, store their embeddings in a vector database, and query them using natural language with AI-powered responses.&#10;&#10;## ✨ Features&#10;&#10;- ** PDF Ingestion**: Load and process PDF documents into searchable chunks&#10;- ** Vector Search**: Powered by Qdrant for fast similarity search&#10;- ** AI-Powered Q&amp;A**: Uses Google Gemini for intelligent question answering&#10;- **⚡ Async Workflows**: Built with Inngest for reliable, observable workflows&#10;- ** Comprehensive Logging**: Full observability with emoji-rich logging&#10;- ** Production Ready**: Type-safe with Pydantic models and error handling&#10;&#10;## ️ Architecture&#10;&#10;```&#10;┌─────────────┐&#10;│   FastAPI   │ ← REST API Entry Point&#10;└──────┬──────┘&#10;       │&#10;       ▼&#10;┌─────────────┐&#10;│   Inngest   │ ← Workflow Engine&#10;└──────┬──────┘&#10;       │&#10;       ├──►  Ingest PDF Workflow&#10;       │    ├─ Load &amp; Chunk PDF&#10;       │    ├─ Generate Embeddings (Gemini)&#10;       │    └─ Store in Qdrant&#10;       │&#10;       └──►  Query PDF Workflow&#10;            ├─ Embed Question (Gemini)&#10;            ├─ Search Vector DB (Qdrant)&#10;            └─ Generate Answer (Gemini)&#10;```&#10;&#10;##  Prerequisites&#10;&#10;- Python 3.13+&#10;- Qdrant (running locally or remote)&#10;- Google Gemini API Key&#10;- Inngest Dev Server (for local development)&#10;&#10;## ️ Installation&#10;&#10;### 1. Clone the Repository&#10;&#10;```bash&#10;git clone &lt;your-repo-url&gt;&#10;cd Prodution_Ready_RAG&#10;```&#10;&#10;### 2. Install Dependencies&#10;&#10;Using `uv` (recommended):&#10;```bash&#10;uv sync&#10;```&#10;&#10;Or using `pip`:&#10;```bash&#10;pip install -r requirements.txt&#10;```&#10;&#10;### 3. Set Up Environment Variables&#10;&#10;Create a `.env` file in the project root:&#10;&#10;```env&#10;# Google Gemini API Key&#10;GEMINI_API_KEY=your_gemini_api_key_here&#10;&#10;# Optional: If using Groq&#10;GROQ_API_KEY=your_groq_api_key_here&#10;&#10;# Qdrant Configuration (optional, defaults shown)&#10;QDRANT_URL=http://localhost:6333&#10;```&#10;&#10;&gt; **Note**: The `.env` file is already in `.gitignore` to keep your secrets safe.&#10;&#10;### 4. Start Qdrant&#10;&#10;#### Using Docker (recommended):&#10;```bash&#10;docker run -p 6333:6333 -p 6334:6334 \&#10;    -v $(pwd)/qdrant_storage:/qdrant/storage:z \&#10;    qdrant/qdrant&#10;```&#10;&#10;#### Or install locally:&#10;```bash&#10;# See: https://qdrant.tech/documentation/quick-start/&#10;```&#10;&#10;### 5. Start Inngest Dev Server&#10;&#10;In a separate terminal:&#10;```bash&#10;npx inngest-cli@latest dev&#10;```&#10;&#10;This will start the Inngest Dev Server at `http://localhost:8288`.&#10;&#10;##  Usage&#10;&#10;### Start the Application&#10;&#10;```bash&#10;uvicorn main:app --reload --log-level info&#10;```&#10;&#10;The API will be available at `http://localhost:8000`.&#10;&#10;### API Endpoints&#10;&#10;#### 1. Health Check&#10;```bash&#10;curl http://localhost:8000/&#10;```&#10;&#10;#### 2. Ingest a PDF&#10;&#10;Send an event to ingest a PDF document:&#10;&#10;```bash&#10;curl -X POST http://localhost:8288/e/rag_app \&#10;  -H &quot;Content-Type: application/json&quot; \&#10;  -d '{&#10;    &quot;name&quot;: &quot;rag/innggest_pdf&quot;,&#10;    &quot;data&quot;: {&#10;      &quot;pdf_path&quot;: &quot;/path/to/your/document.pdf&quot;,&#10;      &quot;source&quot;: &quot;my-document&quot;&#10;    }&#10;  }'&#10;```&#10;&#10;**What happens:**&#10;-  Loads the PDF and splits it into chunks (1000 chars, 200 overlap)&#10;-  Generates 3072-dimensional embeddings using Gemini&#10;-  Stores vectors in Qdrant with metadata&#10;&#10;#### 3. Query the PDF&#10;&#10;Ask questions about your ingested documents:&#10;&#10;```bash&#10;curl -X POST http://localhost:8288/e/rag_app \&#10;  -H &quot;Content-Type: application/json&quot; \&#10;  -d '{&#10;    &quot;name&quot;: &quot;rag/query_pdf_ai&quot;,&#10;    &quot;data&quot;: {&#10;      &quot;question&quot;: &quot;What is the main topic of the document?&quot;,&#10;      &quot;top_k&quot;: 5&#10;    }&#10;  }'&#10;```&#10;&#10;**Response:**&#10;```json&#10;{&#10;  &quot;answer&quot;: &quot;The main topic is...&quot;,&#10;  &quot;sources&quot;: [&quot;my-document&quot;],&#10;  &quot;num_contexts&quot;: 5&#10;}&#10;```&#10;&#10;**What happens:**&#10;-  Embeds your question using Gemini&#10;-  Searches Qdrant for the top 5 most relevant chunks&#10;-  Sends context to Gemini LLM for answer generation&#10;- ✅ Returns the answer with source attribution&#10;&#10;### View Workflow Execution&#10;&#10;Open the Inngest Dev Server UI at `http://localhost:8288` to see:&#10;- Real-time workflow execution&#10;- Step-by-step logs&#10;- Error tracking&#10;- Execution history&#10;&#10;##  Project Structure&#10;&#10;```&#10;Prodution_Ready_RAG/&#10;├── main.py              # FastAPI app + Inngest functions&#10;├── data_loader.py       # PDF loading and embedding logic&#10;├── vector_db.py         # Qdrant vector database wrapper&#10;├── constants.py         # Configuration constants&#10;├── custom_types.py      # Pydantic models&#10;├── .env                 # Environment variables (create this)&#10;├── .gitignore          # Git ignore file&#10;├── pyproject.toml      # Project dependencies&#10;├── README.md           # This file&#10;├── LOGGING_GUIDE.md    # Logging documentation&#10;└── qdrant_storage/     # Qdrant data directory&#10;```&#10;&#10;## ️ Configuration&#10;&#10;Edit `constants.py` to customize:&#10;&#10;```python&#10;# Qdrant&#10;QDRANT_URL = &quot;http://localhost:6333&quot;&#10;COLLECTION_NAME = &quot;docs&quot;&#10;&#10;# Embedding&#10;EMBED_MODEL = &quot;gemini-embedding-001&quot;&#10;EMBEDDING_DIM = 3072&#10;&#10;# Chunking&#10;CHUNK_SIZE = 1000&#10;OVERLAP = 200&#10;&#10;# LLM&#10;GEMINI_LLM_MODEL = &quot;gemini-2.5-flash&quot;&#10;```&#10;&#10;##  Logging&#10;&#10;The system includes comprehensive logging with emoji indicators:&#10;&#10;```&#10;INFO:  Loading PDF: document.pdf&#10;INFO: ✂️ Split into 42 chunks&#10;INFO:  Embedding 42 chunks...&#10;INFO: ✅ Generated 42 vectors (dim=3072)&#10;INFO:  Upserted 42 vectors to Qdrant&#10;INFO:  Searching for question: What is RAG?&#10;INFO: ✅ Embedded question into vector of dim=3072&#10;INFO:  Found 5 contexts&#10;INFO:  Step completed: found 5 contexts from 1 sources&#10;```&#10;&#10;See `LOGGING_GUIDE.md` for more details.&#10;&#10;##  Advanced Usage&#10;&#10;### Custom Embedding Model&#10;&#10;To use a different embedding model:&#10;&#10;1. Update `constants.py`:&#10;```python&#10;EMBED_MODEL = &quot;your-model-name&quot;&#10;EMBEDDING_DIM = your_dimension&#10;```&#10;&#10;2. Modify `data_loader.py` if needed for different API calls&#10;&#10;### Multiple Document Sources&#10;&#10;Track different sources by setting the `source` parameter:&#10;&#10;```json&#10;{&#10;  &quot;name&quot;: &quot;rag/innggest_pdf&quot;,&#10;  &quot;data&quot;: {&#10;    &quot;pdf_path&quot;: &quot;/path/to/doc.pdf&quot;,&#10;    &quot;source&quot;: &quot;research-papers&quot;&#10;  }&#10;}&#10;```&#10;&#10;### Adjust Retrieval&#10;&#10;Modify the `top_k` parameter to retrieve more or fewer context chunks:&#10;&#10;```json&#10;{&#10;  &quot;name&quot;: &quot;rag/query_pdf_ai&quot;,&#10;  &quot;data&quot;: {&#10;    &quot;question&quot;: &quot;Your question?&quot;,&#10;    &quot;top_k&quot;: 10  // Retrieve top 10 chunks&#10;  }&#10;}&#10;```&#10;&#10;&#10;##  Tech Stack&#10;&#10;- **[FastAPI](https://fastapi.tiangolo.com/)** - Modern web framework&#10;- **[Inngest](https://www.inngest.com/)** - Workflow orchestration&#10;- **[Qdrant](https://qdrant.tech/)** - Vector database&#10;- **[Google Gemini](https://ai.google.dev/)** - Embeddings and LLM&#10;- **[LlamaIndex](https://www.llamaindex.ai/)** - PDF parsing and chunking&#10;- **[Pydantic](https://docs.pydantic.dev/)** - Data validation&#10;- **[python-dotenv](https://github.com/theskumar/python-dotenv)** - Environment management&#10;" />
              <option name="updatedContent" value="#  Production Ready RAG System&#10;&#10;A production-grade Retrieval Augmented Generation (RAG) system built with FastAPI, Inngest, Qdrant, and Google Gemini. This system allows you to ingest PDF documents, store their embeddings in a vector database, and query them using natural language with AI-powered responses.&#10;&#10;## ✨ Features&#10;&#10;- ** PDF Ingestion**: Load and process PDF documents into searchable chunks&#10;- ** Vector Search**: Powered by Qdrant for fast similarity search&#10;- ** AI-Powered Q&amp;A**: Uses Google Gemini for intelligent question answering&#10;- **⚡ Async Workflows**: Built with Inngest for reliable, observable workflows&#10;- ** Comprehensive Logging**: Full observability with emoji-rich logging&#10;- ** Production Ready**: Type-safe with Pydantic models and error handling&#10;&#10;## ️ Architecture&#10;&#10;This system uses **[Inngest](https://www.inngest.com/)** - a workflow orchestration platform that provides:&#10;- ⚡ **Event-driven workflows**: Trigger functions via events&#10;-  **Automatic retries**: Built-in error handling and recovery&#10;-  **Observability**: Full visibility into workflow execution&#10;- ⏱️ **Step management**: Break workflows into resumable steps&#10;-  **Type safety**: Pydantic-based serialization&#10;&#10;```&#10;┌─────────────┐&#10;│  Streamlit  │ ← Web UI (Optional)&#10;│     UI      │&#10;└──────┬──────┘&#10;       │&#10;┌──────▼──────┐&#10;│   FastAPI   │ ← REST API Entry Point&#10;└──────┬──────┘&#10;       │&#10;       ▼&#10;┌─────────────┐&#10;│   Inngest   │ ← Workflow Engine (Orchestrates everything)&#10;└──────┬──────┘&#10;       │&#10;       ├──►  Ingest PDF Workflow&#10;       │    ├─ Load &amp; Chunk PDF&#10;       │    ├─ Generate Embeddings (Gemini)&#10;       │    └─ Store in Qdrant&#10;       │&#10;       └──►  Query PDF Workflow&#10;            ├─ Embed Question (Gemini)&#10;            ├─ Search Vector DB (Qdrant)&#10;            └─ Generate Answer (Gemini)&#10;```&#10;&#10;### Why Inngest?&#10;&#10;Unlike traditional REST APIs where you must manage retries, timeouts, and monitoring yourself, Inngest handles:&#10;- **Durable execution**: If a step fails, it retries automatically&#10;- **Step isolation**: Each step runs independently and can be retried&#10;- **Built-in observability**: View every workflow execution in the Inngest dashboard&#10;- **Easy debugging**: See exactly which step failed and why&#10;&#10;##  Prerequisites&#10;&#10;- Python 3.13+&#10;- Qdrant (running locally or remote)&#10;- Google Gemini API Key&#10;- Inngest Dev Server (for local development)&#10;&#10;## ️ Installation&#10;&#10;### 1. Clone the Repository&#10;&#10;```bash&#10;git clone &lt;your-repo-url&gt;&#10;cd Prodution_Ready_RAG&#10;```&#10;&#10;### 2. Install Dependencies&#10;&#10;Using `uv` (recommended):&#10;```bash&#10;uv sync&#10;```&#10;&#10;Or using `pip`:&#10;```bash&#10;pip install -r requirements.txt&#10;```&#10;&#10;### 3. Set Up Environment Variables&#10;&#10;Create a `.env` file in the project root:&#10;&#10;```env&#10;# Google Gemini API Key&#10;GEMINI_API_KEY=your_gemini_api_key_here&#10;&#10;# Optional: If using Groq&#10;GROQ_API_KEY=your_groq_api_key_here&#10;&#10;# Qdrant Configuration (optional, defaults shown)&#10;QDRANT_URL=http://localhost:6333&#10;```&#10;&#10;&gt; **Note**: The `.env` file is already in `.gitignore` to keep your secrets safe.&#10;&#10;### 4. Start Qdrant&#10;&#10;#### Using Docker (recommended):&#10;```bash&#10;docker run -p 6333:6333 -p 6334:6334 \&#10;    -v $(pwd)/qdrant_storage:/qdrant/storage:z \&#10;    qdrant/qdrant&#10;```&#10;&#10;#### Or install locally:&#10;```bash&#10;# See: https://qdrant.tech/documentation/quick-start/&#10;```&#10;&#10;### 5. Start Inngest Dev Server&#10;&#10;In a separate terminal:&#10;```bash&#10;npx inngest-cli@latest dev&#10;```&#10;&#10;This will start the Inngest Dev Server at `http://localhost:8288`.&#10;&#10;##  Usage&#10;&#10;### Start the Application&#10;&#10;```bash&#10;uvicorn main:app --reload --log-level info&#10;```&#10;&#10;The API will be available at `http://localhost:8000`.&#10;&#10;### Quick Start with Streamlit UI &#10;&#10;**For the easiest experience, use the Streamlit UI!**&#10;&#10;```bash&#10;# Make sure all services are running (Qdrant, Inngest, FastAPI)&#10;# Then start the Streamlit UI:&#10;streamlit run streamlit_app.py&#10;```&#10;&#10;Open `http://localhost:8501` in your browser. You'll see:&#10;1. ** Upload Section**: Drag and drop your PDF file&#10;2. ** Chat Interface**: Type your question in natural language&#10;3. ** Results**: See the AI's answer with source citations&#10;4. **⚙️ Settings**: Adjust retrieval parameters (top_k)&#10;&#10;**No curl commands needed!** Perfect for demos and non-technical users.&#10;&#10;---&#10;&#10;### API Endpoints&#10;&#10;#### 1. Health Check&#10;```bash&#10;curl http://localhost:8000/&#10;```&#10;&#10;#### 2. Ingest a PDF&#10;&#10;Send an event to ingest a PDF document:&#10;&#10;```bash&#10;curl -X POST http://localhost:8288/e/rag_app \&#10;  -H &quot;Content-Type: application/json&quot; \&#10;  -d '{&#10;    &quot;name&quot;: &quot;rag/innggest_pdf&quot;,&#10;    &quot;data&quot;: {&#10;      &quot;pdf_path&quot;: &quot;/path/to/your/document.pdf&quot;,&#10;      &quot;source&quot;: &quot;my-document&quot;&#10;    }&#10;  }'&#10;```&#10;&#10;**What happens:**&#10;-  Loads the PDF and splits it into chunks (1000 chars, 200 overlap)&#10;-  Generates 3072-dimensional embeddings using Gemini&#10;-  Stores vectors in Qdrant with metadata&#10;&#10;#### 3. Query the PDF&#10;&#10;Ask questions about your ingested documents:&#10;&#10;```bash&#10;curl -X POST http://localhost:8288/e/rag_app \&#10;  -H &quot;Content-Type: application/json&quot; \&#10;  -d '{&#10;    &quot;name&quot;: &quot;rag/query_pdf_ai&quot;,&#10;    &quot;data&quot;: {&#10;      &quot;question&quot;: &quot;What is the main topic of the document?&quot;,&#10;      &quot;top_k&quot;: 5&#10;    }&#10;  }'&#10;```&#10;&#10;**Response:**&#10;```json&#10;{&#10;  &quot;answer&quot;: &quot;The main topic is...&quot;,&#10;  &quot;sources&quot;: [&quot;my-document&quot;],&#10;  &quot;num_contexts&quot;: 5&#10;}&#10;```&#10;&#10;**What happens:**&#10;-  Embeds your question using Gemini&#10;-  Searches Qdrant for the top 5 most relevant chunks&#10;-  Sends context to Gemini LLM for answer generation&#10;- ✅ Returns the answer with source attribution&#10;&#10;### 4. Use the Streamlit UI (Recommended for Easy Testing)&#10;&#10;For a user-friendly interface, you can use the Streamlit UI:&#10;&#10;```bash&#10;streamlit run streamlit_app.py&#10;```&#10;&#10;The UI will open at `http://localhost:8501` and provides:&#10;-  **File Upload**: Drag and drop PDF files to ingest&#10;-  **Chat Interface**: Ask questions naturally&#10;-  **Source Display**: See which document chunks were used&#10;- ⚙️ **Configuration**: Adjust `top_k` and other parameters&#10;-  **Visual Feedback**: Real-time status updates and loading indicators&#10;&#10;**Benefits of the Streamlit UI:**&#10;- No need to write curl commands&#10;- Interactive file upload&#10;- Instant visual feedback&#10;- Better for demos and testing&#10;- User-friendly for non-technical users&#10;&#10;### View Workflow Execution&#10;&#10;Open the Inngest Dev Server UI at `http://localhost:8288` to see:&#10;- Real-time workflow execution&#10;- Step-by-step logs&#10;- Error tracking&#10;- Execution history&#10;- Retry attempts and failures&#10;- Detailed timing information&#10;&#10;##  Project Structure&#10;&#10;```&#10;Prodution_Ready_RAG/&#10;├── main.py              # FastAPI app + Inngest functions&#10;├── streamlit_app.py     # Streamlit UI (optional, user-friendly interface)&#10;├── data_loader.py       # PDF loading and embedding logic&#10;├── vector_db.py         # Qdrant vector database wrapper&#10;├── constants.py         # Configuration constants&#10;├── custom_types.py      # Pydantic models&#10;├── .env                 # Environment variables (create this)&#10;├── .gitignore          # Git ignore file&#10;├── pyproject.toml      # Project dependencies&#10;├── README.md           # This file&#10;├── LOGGING_GUIDE.md    # Logging documentation&#10;└── qdrant_storage/     # Qdrant data directory&#10;```&#10;&#10;## ️ Configuration&#10;&#10;Edit `constants.py` to customize:&#10;&#10;```python&#10;# Qdrant&#10;QDRANT_URL = &quot;http://localhost:6333&quot;&#10;COLLECTION_NAME = &quot;docs&quot;&#10;&#10;# Embedding&#10;EMBED_MODEL = &quot;gemini-embedding-001&quot;&#10;EMBEDDING_DIM = 3072&#10;&#10;# Chunking&#10;CHUNK_SIZE = 1000&#10;OVERLAP = 200&#10;&#10;# LLM&#10;GEMINI_LLM_MODEL = &quot;gemini-2.5-flash&quot;&#10;```&#10;&#10;##  Logging&#10;&#10;The system includes comprehensive logging with emoji indicators:&#10;&#10;```&#10;INFO:  Loading PDF: document.pdf&#10;INFO: ✂️ Split into 42 chunks&#10;INFO:  Embedding 42 chunks...&#10;INFO: ✅ Generated 42 vectors (dim=3072)&#10;INFO:  Upserted 42 vectors to Qdrant&#10;INFO:  Searching for question: What is RAG?&#10;INFO: ✅ Embedded question into vector of dim=3072&#10;INFO:  Found 5 contexts&#10;INFO:  Step completed: found 5 contexts from 1 sources&#10;```&#10;&#10;See `LOGGING_GUIDE.md` for more details.&#10;&#10;##  Advanced Usage&#10;&#10;### Custom Embedding Model&#10;&#10;To use a different embedding model:&#10;&#10;1. Update `constants.py`:&#10;```python&#10;EMBED_MODEL = &quot;your-model-name&quot;&#10;EMBEDDING_DIM = your_dimension&#10;```&#10;&#10;2. Modify `data_loader.py` if needed for different API calls&#10;&#10;### Multiple Document Sources&#10;&#10;Track different sources by setting the `source` parameter:&#10;&#10;```json&#10;{&#10;  &quot;name&quot;: &quot;rag/innggest_pdf&quot;,&#10;  &quot;data&quot;: {&#10;    &quot;pdf_path&quot;: &quot;/path/to/doc.pdf&quot;,&#10;    &quot;source&quot;: &quot;research-papers&quot;&#10;  }&#10;}&#10;```&#10;&#10;### Adjust Retrieval&#10;&#10;Modify the `top_k` parameter to retrieve more or fewer context chunks:&#10;&#10;```json&#10;{&#10;  &quot;name&quot;: &quot;rag/query_pdf_ai&quot;,&#10;  &quot;data&quot;: {&#10;    &quot;question&quot;: &quot;Your question?&quot;,&#10;    &quot;top_k&quot;: 10  // Retrieve top 10 chunks&#10;  }&#10;}&#10;```&#10;&#10;&#10;##  Tech Stack&#10;&#10;- **[FastAPI](https://fastapi.tiangolo.com/)** - Modern web framework&#10;- **[Inngest](https://www.inngest.com/)** - Workflow orchestration&#10;- **[Qdrant](https://qdrant.tech/)** - Vector database&#10;- **[Google Gemini](https://ai.google.dev/)** - Embeddings and LLM&#10;- **[LlamaIndex](https://www.llamaindex.ai/)** - PDF parsing and chunking&#10;- **[Streamlit](https://streamlit.io/)** - Interactive web UI&#10;- **[Pydantic](https://docs.pydantic.dev/)** - Data validation&#10;- **[python-dotenv](https://github.com/theskumar/python-dotenv)** - Environment management&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>