# ğŸš€ Production Ready RAG System

A production-grade Retrieval Augmented Generation (RAG) system built with FastAPI, Inngest, Qdrant, and Google Gemini. This system allows you to ingest PDF documents, store their embeddings in a vector database, and query them using natural language with AI-powered responses.

## âœ¨ Features

- **ğŸ“„ PDF Ingestion**: Load and process PDF documents into searchable chunks
- **ğŸ” Vector Search**: Powered by Qdrant for fast similarity search
- **ğŸ¤– AI-Powered Q&A**: Uses Google Gemini for intelligent question answering
- **âš¡ Async Workflows**: Built with Inngest for reliable, observable workflows
- **ğŸ“Š Comprehensive Logging**: Full observability with emoji-rich logging
- **ğŸ¯ Production Ready**: Type-safe with Pydantic models and error handling

## ğŸ—ï¸ Architecture

This system uses **[Inngest](https://www.inngest.com/)** - a workflow orchestration platform that provides:
- âš¡ **Event-driven workflows**: Trigger functions via events
- ğŸ”„ **Automatic retries**: Built-in error handling and recovery
- ğŸ“Š **Observability**: Full visibility into workflow execution
- â±ï¸ **Step management**: Break workflows into resumable steps
- ğŸ¯ **Type safety**: Pydantic-based serialization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit  â”‚ â† Web UI (Optional)
â”‚     UI      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI   â”‚ â† REST API Entry Point
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Inngest   â”‚ â† Workflow Engine (Orchestrates everything)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â–º ğŸ“– Ingest PDF Workflow
       â”‚    â”œâ”€ Load & Chunk PDF
       â”‚    â”œâ”€ Generate Embeddings (Gemini)
       â”‚    â””â”€ Store in Qdrant
       â”‚
       â””â”€â”€â–º ğŸ” Query PDF Workflow
            â”œâ”€ Embed Question (Gemini)
            â”œâ”€ Search Vector DB (Qdrant)
            â””â”€ Generate Answer (Gemini)
```

### Why Inngest?

Unlike traditional REST APIs where you must manage retries, timeouts, and monitoring yourself, Inngest handles:
- **Durable execution**: If a step fails, it retries automatically
- **Step isolation**: Each step runs independently and can be retried
- **Built-in observability**: View every workflow execution in the Inngest dashboard
- **Easy debugging**: See exactly which step failed and why

## ğŸ“‹ Prerequisites

- Python 3.13+
- Qdrant (running locally or remote)
- Google Gemini API Key
- Inngest Dev Server (for local development)

## ğŸ› ï¸ Installation

### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd Prodution_Ready_RAG
```

### 2. Install Dependencies

Using `uv` (recommended):
```bash
uv sync
```

Or using `pip`:
```bash
pip install -r requirements.txt
```

### 3. Set Up Environment Variables

Create a `.env` file in the project root:

```env
# Google Gemini API Key
GEMINI_API_KEY=your_gemini_api_key_here

# Optional: If using Groq
GROQ_API_KEY=your_groq_api_key_here

# Qdrant Configuration (optional, defaults shown)
QDRANT_URL=http://localhost:6333
```

> **Note**: The `.env` file is already in `.gitignore` to keep your secrets safe.

### 4. Start Qdrant

#### Using Docker (recommended):
```bash
docker run -p 6333:6333 -p 6334:6334 \
    -v $(pwd)/qdrant_storage:/qdrant/storage:z \
    qdrant/qdrant
```

#### Or install locally:
```bash
# See: https://qdrant.tech/documentation/quick-start/
```

### 5. Start Inngest Dev Server

In a separate terminal:
```bash
npx inngest-cli@latest dev
```

This will start the Inngest Dev Server at `http://localhost:8288`.

## ğŸš€ Usage

### Start the Application

```bash
uvicorn main:app --reload --log-level info
```

The API will be available at `http://localhost:8000`.

### Quick Start with Streamlit UI ğŸ¨

**For the easiest experience, use the Streamlit UI!**

```bash
# Make sure all services are running (Qdrant, Inngest, FastAPI)
# Then start the Streamlit UI:
streamlit run streamlit_app.py
```

Open `http://localhost:8501` in your browser. You'll see:
1. **ğŸ“¤ Upload Section**: Drag and drop your PDF file
2. **ğŸ’¬ Chat Interface**: Type your question in natural language
3. **ğŸ“Š Results**: See the AI's answer with source citations
4. **âš™ï¸ Settings**: Adjust retrieval parameters (top_k)

**No curl commands needed!** Perfect for demos and non-technical users.

---

### API Endpoints

#### 1. Health Check
```bash
curl http://localhost:8000/
```

#### 2. Ingest a PDF

Send an event to ingest a PDF document:

```bash
curl -X POST http://localhost:8288/e/rag_app \
  -H "Content-Type: application/json" \
  -d '{
    "name": "rag/innggest_pdf",
    "data": {
      "pdf_path": "/path/to/your/document.pdf",
      "source": "my-document"
    }
  }'
```

**What happens:**
- ğŸ“– Loads the PDF and splits it into chunks (1000 chars, 200 overlap)
- ğŸ”¢ Generates 3072-dimensional embeddings using Gemini
- ğŸ’¾ Stores vectors in Qdrant with metadata

#### 3. Query the PDF

Ask questions about your ingested documents:

```bash
curl -X POST http://localhost:8288/e/rag_app \
  -H "Content-Type: application/json" \
  -d '{
    "name": "rag/query_pdf_ai",
    "data": {
      "question": "What is the main topic of the document?",
      "top_k": 5
    }
  }'
```

**Response:**
```json
{
  "answer": "The main topic is...",
  "sources": ["my-document"],
  "num_contexts": 5
}
```

**What happens:**
- ğŸ” Embeds your question using Gemini
- ğŸ“Š Searches Qdrant for the top 5 most relevant chunks
- ğŸ¤– Sends context to Gemini LLM for answer generation
- âœ… Returns the answer with source attribution

### 4. Use the Streamlit UI (Recommended for Easy Testing)

For a user-friendly interface, you can use the Streamlit UI:

```bash
streamlit run streamlit_app.py
```

The UI will open at `http://localhost:8501` and provides:
- ğŸ“¤ **File Upload**: Drag and drop PDF files to ingest
- ğŸ’¬ **Chat Interface**: Ask questions naturally
- ğŸ“Š **Source Display**: See which document chunks were used
- âš™ï¸ **Configuration**: Adjust `top_k` and other parameters
- ğŸ¨ **Visual Feedback**: Real-time status updates and loading indicators

**Benefits of the Streamlit UI:**
- No need to write curl commands
- Interactive file upload
- Instant visual feedback
- Better for demos and testing
- User-friendly for non-technical users

### View Workflow Execution

Open the Inngest Dev Server UI at `http://localhost:8288` to see:
- Real-time workflow execution
- Step-by-step logs
- Error tracking
- Execution history
- Retry attempts and failures
- Detailed timing information

## ğŸ“ Project Structure

```
Prodution_Ready_RAG/
â”œâ”€â”€ main.py              # FastAPI app + Inngest functions
â”œâ”€â”€ streamlit_app.py     # Streamlit UI (optional, user-friendly interface)
â”œâ”€â”€ data_loader.py       # PDF loading and embedding logic
â”œâ”€â”€ vector_db.py         # Qdrant vector database wrapper
â”œâ”€â”€ constants.py         # Configuration constants
â”œâ”€â”€ custom_types.py      # Pydantic models
â”œâ”€â”€ .env                 # Environment variables (create this)
â”œâ”€â”€ .gitignore          # Git ignore file
â”œâ”€â”€ pyproject.toml      # Project dependencies
â”œâ”€â”€ README.md           # This file
â”œâ”€â”€ LOGGING_GUIDE.md    # Logging documentation
â””â”€â”€ qdrant_storage/     # Qdrant data directory
```

## ğŸ›ï¸ Configuration

Edit `constants.py` to customize:

```python
# Qdrant
QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "docs"

# Embedding
EMBED_MODEL = "gemini-embedding-001"
EMBEDDING_DIM = 3072

# Chunking
CHUNK_SIZE = 1000
OVERLAP = 200

# LLM
GEMINI_LLM_MODEL = "gemini-2.5-flash"
```

## ğŸ“Š Logging

The system includes comprehensive logging with emoji indicators:

```
INFO: ğŸ“– Loading PDF: document.pdf
INFO: âœ‚ï¸ Split into 42 chunks
INFO: ğŸ”¢ Embedding 42 chunks...
INFO: âœ… Generated 42 vectors (dim=3072)
INFO: ğŸ’¾ Upserted 42 vectors to Qdrant
INFO: ğŸ” Searching for question: What is RAG?
INFO: âœ… Embedded question into vector of dim=3072
INFO: ğŸ“Š Found 5 contexts
INFO: ğŸ¯ Step completed: found 5 contexts from 1 sources
```

See `LOGGING_GUIDE.md` for more details.

## ğŸ”§ Advanced Usage

### Custom Embedding Model

To use a different embedding model:

1. Update `constants.py`:
```python
EMBED_MODEL = "your-model-name"
EMBEDDING_DIM = your_dimension
```

2. Modify `data_loader.py` if needed for different API calls

### Multiple Document Sources

Track different sources by setting the `source` parameter:

```json
{
  "name": "rag/innggest_pdf",
  "data": {
    "pdf_path": "/path/to/doc.pdf",
    "source": "research-papers"
  }
}
```

### Adjust Retrieval

Modify the `top_k` parameter to retrieve more or fewer context chunks:

```json
{
  "name": "rag/query_pdf_ai",
  "data": {
    "question": "Your question?",
    "top_k": 10  // Retrieve top 10 chunks
  }
}
```


## ğŸ“š Tech Stack

- **[FastAPI](https://fastapi.tiangolo.com/)** - Modern web framework
- **[Inngest](https://www.inngest.com/)** - Workflow orchestration
- **[Qdrant](https://qdrant.tech/)** - Vector database
- **[Google Gemini](https://ai.google.dev/)** - Embeddings and LLM
- **[LlamaIndex](https://www.llamaindex.ai/)** - PDF parsing and chunking
- **[Streamlit](https://streamlit.io/)** - Interactive web UI
- **[Pydantic](https://docs.pydantic.dev/)** - Data validation
- **[python-dotenv](https://github.com/theskumar/python-dotenv)** - Environment management
